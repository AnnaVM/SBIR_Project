{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project Proposal: SBIR award prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder from first proposal --> High level view:\n",
    "The Small Business Innovation Research (SBIR,  https://www.sbir.gov/) program relies on an award-based system to trigger high-tech innovation in small US companies. Many agencies (Departement of Agriculture, of Defense, ...) contribute to this fund. Two main phases structure the SBIR program: phase I (approx 150 000 dollars, 6 months) and phase II (approx 1 000 000 dollars for 2 years). To apply to phase II, companies must be phase I awardees. \n",
    "\n",
    "The predictive question I want to answer is: will your project make it to phase II?  \n",
    "The interpretative question I would like to address is: what features make you likely to be successful in applying to phase II?\n",
    "\n",
    "####Aim of this document --> establish the presence of a signal in the data:\n",
    "This document follows the pipeline I developped to evaluate a simple predictive model (Logistic Regression) on a small dataset. In the end, this exploratory work shows that the simple predictor outperforms a baseline predictor (one that randomly predicts success or failure with the baseline probability of the training set). Further ideas to increase predictive power are also listed.\n",
    "\n",
    "Data and code I used for this proposal can be found on GitHub, at https://github.com/AnnaVM/SBIR_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#from code in the GitHub Repro (https://github.com/AnnaVM/SBIR_Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/AnnaVMS/Desktop/Galvanize/SBIR-project/code\n"
     ]
    }
   ],
   "source": [
    "cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from prepare_data import subset_data\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/AnnaVMS/Desktop/Galvanize/SBIR-project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Loading a subset of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this proposal, I worked only on a subset of my data (data from years 2012 to 2015, with one specific agency, the Department of Defense 'dod'). \n",
    "- A first converter allowed me to go from .xlsx files to .csv files. (not shown here, available on the GitHub Repo)\n",
    "- A second function allows me to work on the subset (code and csv files are available on the GitHub Repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015-12-14_award_export_10001TO15000.csv', '2015-12-14_award_export_1TO5000.csv', '2015-12-14_award_export_5001TO10000.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prepare_data.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df = subset_data('dod', 2012, '/Users/AnnaVMS/Desktop/test2')\n",
    "#update path according to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of phase I projects considered:  1016\n"
     ]
    }
   ],
   "source": [
    "print 'number of phase I projects considered: ',len(df.to_phase_II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of phase I projects with successful application to phase II:  305\n"
     ]
    }
   ],
   "source": [
    "print 'number of phase I projects with successful application to phase II: ',sum(df.to_phase_II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of phase I projects without a phase II:  711\n"
     ]
    }
   ],
   "source": [
    "print 'number of phase I projects without a phase II: ',len(df.to_phase_II)-sum(df.to_phase_II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information in the database: \n",
      "Index([u'index', u'Company', u'Award Title', u'Agency', u'Branch', u'Phase',\n",
      "       u'Program', u'Agency Tracking #', u'Contract', u'Award Start Date',\n",
      "       u'Award Close Date', u'Solicitation #', u'Solicitation Year',\n",
      "       u'Topic Code', u'Award Year', u'Award Amount', u'DUNS',\n",
      "       u'Hubzone Owned', u'Socially and Economically Disadvantaged',\n",
      "       u'Woman Owned', u'# Employees', u'Company Website', u'Address1',\n",
      "       u'Address2', u'City', u'State', u'Zip', u'Contact Name',\n",
      "       u'Contact Title', u'Contact Phone', u'Contact Email', u'PI Name',\n",
      "       u'PI Title', u'PI Phone', u'PI Email', u'RI Name', u'RI POC Name',\n",
      "       u'RI POC Phone', u'Research Keywords', u'Abstract', u'to_phase_II'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#quick overview of the dataset\n",
    "print 'information in the database: \\n',df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The last column 'to_phase_II' gives information on whether a phase II was obtained. It will be used as the label to predict in my model\n",
    "- Features can be choosen in the rest of the columns (here for instance: 'Solicitation Year', 'Award Amount', 'Hubzone Owned', 'Socially and Economically Disadvantaged', 'Woman Owned', '# Employees') or engineered (from 'Abstract', I made 'Abstract Length'--simple character count-- and 'Topic of Abstract' --through NMF topic modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###defining a first model, based on this subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining a training and testing set (through indices)\n",
    "kf = KFold(1016,5, shuffle=True)\n",
    "kf_iterator = kf.__iter__()\n",
    "train_index, test_index = kf_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The Pipeline:\n",
    "1- Processing the text of the Abstract of submissions\n",
    "- tfidf vectorization of the text (5000 words in the dictionary, default tokenization of sklearn)\n",
    "- topic modeling: NMF (5 components, no optimization done here)\n",
    "\n",
    "2- Preparing data for model\n",
    "- using dummy variables for topics\n",
    "- mapping Y/N to 1/0\n",
    "- getting the abstract length in characters\n",
    "\n",
    "3- Running the Logistic Regression\n",
    "- Standard scaling of the data\n",
    "- Logistic Regression (lasso regularization)\n",
    "--> outputs the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test = Model(df, train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 1\n",
    "model_test.process_text('Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 2\n",
    "model_test.prepare_LogReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.73891625615763545, 0.58019414662416691, 0.74509803921568629)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 3\n",
    "model_test.perform_LogReg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first score is the accuracy of the Logistic Classifier on the training set;\n",
    "- the second score is the accuracy of the baseline predictor (one that randomly predicts success or failure with the baseline probability of the training set),\n",
    "- the third score gives the accuracy of the Logistic Classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = model_test.LogReg_ytest\n",
    "predicted_labels = model_test.model_LogReg.predict(model_test.LogReg_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tp_fp(labels, predicted_labels):\n",
    "    labels = np.array(labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    tp = sum((labels == predicted_labels) & (labels == 1))\n",
    "    tn = sum((labels == predicted_labels) & (labels == 0))\n",
    "    fp = sum((labels != predicted_labels) & (predicted_labels == 1))\n",
    "    fn = sum((labels != predicted_labels) & (predicted_labels == 0))\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = tp_fp(labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "print sum(model_test.LogReg_ytest)\n",
    "print len(model_test.LogReg_ytest)-sum(model_test.LogReg_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 130 13 39\n"
     ]
    }
   ],
   "source": [
    "print tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.745098039216 0.360655737705 0.628571428571\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn)*1./(tp+tn+fp+fn)\n",
    "recall = tp*1./(tp + fn)\n",
    "precision = tp*1./(tp + fp)\n",
    "print accuracy, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(model_test.LogReg_Xtrain, model_test.LogReg_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7009803921568627"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(model_test.LogReg_Xtest, model_test.LogReg_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_rf_labels = rf.predict(model_test.LogReg_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = tp_fp(labels, predicted_rf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.700980392157 0.377049180328 0.5\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn)*1./(tp+tn+fp+fn)\n",
    "recall = tp*1./(tp + fn)\n",
    "precision = tp*1./(tp + fp)\n",
    "print accuracy, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Some information on the models (coefficients from LogReg and topics from NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coeffs</th>\n",
       "      <th>criterion:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>topic 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.010458</td>\n",
       "      <td>topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.183616</td>\n",
       "      <td>topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.173449</td>\n",
       "      <td>topic 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.108927</td>\n",
       "      <td>topic 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.009670</td>\n",
       "      <td>Solicitation Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.695898</td>\n",
       "      <td>Award Amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.076544</td>\n",
       "      <td>Hubzone Owned as_int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.098911</td>\n",
       "      <td>Socially and Economically Disadvantaged as_int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.106902</td>\n",
       "      <td>Woman Owned as_int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.127002</td>\n",
       "      <td># Employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.666224</td>\n",
       "      <td>Abstract Length</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      coeffs                                      criterion:\n",
       "0   0.000000                                         topic 4\n",
       "1  -0.010458                                         topic 3\n",
       "2  -0.183616                                         topic 2\n",
       "3   0.173449                                         topic 1\n",
       "4   0.108927                                         topic 0\n",
       "5   0.009670                               Solicitation Year\n",
       "6   0.695898                                    Award Amount\n",
       "7   0.076544                            Hubzone Owned as_int\n",
       "8  -0.098911  Socially and Economically Disadvantaged as_int\n",
       "9   0.106902                              Woman Owned as_int\n",
       "10 -0.127002                                     # Employees\n",
       "11  0.666224                                 Abstract Length"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the coefficients in the Logisitic Regression (lasso regularized)\n",
    "pd.DataFrame( {'criterion:': model_test.list_columns[:-1], \n",
    "               'coeffs': model_test.model_LogReg.coef_[0]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'sensor' u'data' u'sensors' u'space' u'tracking' u'target' u'objects'\n",
      " u'algorithms' u'radar' u'detection']\n",
      "[u'power' u'high' u'energy' u'technology' u'fuel' u'design' u'low'\n",
      " u'applications' u'antenna' u'performance']\n",
      "[u'training' u'data' u'information' u'network' u'analysis' u'security'\n",
      " u'learning' u'cyber' u'support' u'based']\n",
      "[u'laser' u'optical' u'high' u'lasers' u'wavelength' u'diode' u'spectral'\n",
      " u'swir' u'silicon' u'fiber']\n",
      "[u'model' u'models' u'aircraft' u'materials' u'damage' u'modeling' u'phase'\n",
      " u'process' u'plume' u'software']\n"
     ]
    }
   ],
   "source": [
    "#topics defined with NMF:\n",
    "for i in xrange(5):\n",
    "    print model_test.get_top_n_words_for_topic(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Further Work\n",
    "\n",
    "Other, easy but time-consuming developments\n",
    "- adding other features (State...)\n",
    "- vectorizing and topic modeling for other text inputs (Title, Keywords, Contact Title...)\n",
    "- adding further company information\n",
    "- tuning regularization on Logistic Regression\n",
    "- running other models (getting feature importance with Random Forest for interpretation, trying to increase predictive power with SVM)\n",
    "\n",
    "Other, harder, routes:\n",
    "- developping a sentiment analysis (very technical, very assured, very detail-oriented, very goal-oriented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bibliography\n",
    "A paper featured in Kaggle, as a source inspiration\n",
    "- http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
